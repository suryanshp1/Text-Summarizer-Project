# TrainingArguments:
#   num_train_epochs: 1
#   warmup_steps: 500
#   per_device_train_batch_size: 1
#   weight_decay: 0.01
#   logging_steps: 10
#   evaluation_strategy: steps
#   eval_steps: 500
#   save_steps: 1e3
#   gradient_accumulation_steps: 16

TrainingArguments:
  num_train_epochs: 1
  warmup_steps: 1
  per_device_train_batch_size: 1  # Keep as is, as it's already minimal
  weight_decay: 0.01
  logging_steps: 1
  evaluation_strategy: steps
  eval_steps: 1  # Increase eval steps if needed to reduce frequency
  save_steps: 1  # Save less frequently
  gradient_accumulation_steps: 1  # Reduce to lessen memory usage